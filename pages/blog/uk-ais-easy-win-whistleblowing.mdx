---
title: "An easy win for UK AI safety: supporting whistleblowers"
publishedOn: "2024-09-11"
---

tldr: We could improve AI safety by updating UK whistleblowing laws to cover AI risks. This would involve amending the Public Interest Disclosure Act 1998 to include AI-related disclosures and designating DSIT as a prescribed body for receiving these disclosures.

## Motivation

Employees, contractors, and auditors working closely with AI systems are often best positioned to identify emerging AI risks. However, they may hesitate to report concerns due to fear of retaliation.

For example, OpenAI researcher Daniel Kokotajlo expected to lose $1.7 million, or 85% of his family’s net worth, by refusing to sign an agreement that would permanently bar him from reporting concerns to a regulator. After this became public OpenAI changed these contract terms, but the incident highlights the broader need for robust whistleblower protections in the AI industry.

## Current laws

_Disclaimer: this is commentary from somebody on the internet, not legal advice!_

The UK's primary whistleblowing legislation is the Public Interest Disclosure Act 1998, which amends the Employment Rights Act 1996. This law protects workers making "qualifying disclosures" about certain types of wrongdoing. These are defined in section 43B of the Employment Rights Act and include reporting criminal offences, failures to comply with legal obligations, and damage to the environment.

However, because the UK government has not legislated on AI yet, there aren’t many existing legal obligations to comply with.

The other exceptions for qualifying disclosures don’t obviously apply.[^1] Even where it’s debatable, it’d still likely result in a complex court battle. This would almost certainly be in the interests of AI companies with billions of dollars of investment, and probably not for individuals who might have already been stripped of a lot of their income.

## Bridging the gap

To address these issues, we could make the following amendments to the Public Interest Disclosure Act 1998:

### Adding a prescribed person

The Secretary of State should designate the Department for Science, Innovation and Technology (DSIT) as a prescribed person for emerging technology disclosures under section 43F. This can be done with a statutory order without parliament’s approval.

In future the responsibility for regulating AI might be moved to a separate regulator, like [Lord Holme’s proposed AI Authority](https://bills.parliament.uk/bills/3519). In this case this body could be named a prescribed person.

### Adding disclosure categories

Add two new categories of qualifying disclosure under section 43B(1):

> (g) that the development or deployment of an emerging technology has caused harm, is causing harm, or is likely to cause harm, or
>
> (h) there is a credible risk of catastrophic or extreme harm, including but not limited to threats to national security and public safety

(These are a first draft by someone without a formal legal background! I’m sure they can be refined by real lawyers - but the point is to get something like this in.)

These additions would explicitly protect workers who report AI-related risks that don't fall under existing categories. They’ll also help avoid getting into this situation again, where legal obligations don’t yet exist for potentially harmful emerging technologies.

There aren’t special powers to make this change, so it would require parliament’s approval.

## Next steps

Again, to strengthen AI safety whistleblowing protections:

1. The Secretary of State should add DSIT to the list of prescribed persons.

2. The government should draft amendments to the Public Interest Disclosure Act 1998 to add the disclosure categories. Parliament should then debate the amendments to update the whistleblowing law.

These changes would create a safer environment for reporting AI risks, potentially averting serious harms. They’d also contribute to the responsible development of all emerging technologies in the UK.

_If you enjoyed this article, you might also like [The AI regulator’s toolbox: A list of concrete AI governance practices](../ai-regulator-toolbox/)._

{/* _If you enjoyed this article, you might also like [An easy win for UK AI safety: competition law safe harbour](../uk-ais-easy-win-competition-safe-harbour/)._ */}

[^1]:
    The potentially relevant bases are listed in [section 1 of the Public Interest Disclosure Act](https://www.legislation.gov.uk/ukpga/1998/23/section/1), inserted as [section 43B of the Employment Rights Act 1996](https://www.legislation.gov.uk/ukpga/1996/18/section/43B). These include:
    * “that the health or safety of any individual has been, is being or is likely to be endangered”: arguably this could include the personal security of individuals, of which a dangerous AI system could endanger.
    * “that the environment has been, is being or is likely to be damaged”: the ‘environment’ is actually not defined anywhere in the act. It’s arguable whether this effectively is everything around us, in which case a dangerous AI could be likely to damage something in the world.

    I am not a lawyer and if you’re planning to go to battle with a company with billions to spend I’d recommend getting better advice than a footnote in a blog post by someone who doesn’t know your specific situation. [Protect](https://protect-advice.org.uk/) might be a good starting point for those based in the UK.
