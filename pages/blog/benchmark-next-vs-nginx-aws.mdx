---
title: "Follow-up: benchmarking Next.js server vs nginx at serving a static site, now on AWS"
publishedOn: "2024-04-30"
---

import Details from '../../components/Details';
import BarChart from '../../components/BarChart';

Yesterday, I published [a benchmark that compared a few different ways of hosting a static Next.js site](../benchmark-next-vs-nginx/). I tested against a server running locally to focus as much as possible on the performance difference between the tools alone.

However, in the real world your servers are far away!

I wondered how this would affect the test results. My main assumption would be that it just adds some constant to all of them. But why not test, given I’ve got almost everything so easily set up for that now?

I spun up the same setup on a t2.micro instance running Ubuntu 24.04 on AWS EC2[^1], and re-ran the tests.

The first time I did this, I got very weird results. I realised this was because Next.js was just dropping a lot of the requests. I decreased the rate from 5000 to 100 to help it better cope with this.

Here are the results after making this change:

Median HTTP load times, high reqs/second (i.e. for the HTML response, using [k6](https://grafana.com/docs/k6/latest/get-started/running-k6/)):

<BarChart data={[
  ['Deployment type', 'Load time, lower is better', {role: 'annotation'}],
  ['nginx', 17.1585, '17ms'],
  ['next start', 18.24, '18ms'],
  ['next dev', 42.709, '43ms'],
]} />

Median page load times, no additional reqs/second (i.e. for the browser to consider the page fully loaded, using [Puppeteer](https://pptr.dev/)):

<BarChart data={[
  ['Deployment type', 'Load time, lower is better', {role: 'annotation'}],
  ['nginx', 107.971812, '108ms'],
  ['next start', 102.241167, '102ms'],
  ['next dev', 357.8632295, '358ms'],
]} />

<Details title='Full results'>

<div className='overflow-x-auto'>
|                     | nginx       | next&nbsp;start | next&nbsp;dev |
|---------------------|-------------|-----------------|---------------|
| load_passes         |         500 |             500 |           500 |
| load_avg            | 126.0101826 |      106.574511 |   388.8905562 |
| load_med            |  107.971812 |      102.241167 |   357.8632295 |
| load_p90            | 212.3881503 |     116.0010872 |   488.5655202 |
| load_p95            | 227.4584729 |     127.3348625 |   556.4741458 |
| req_passes          |        5903 |            5701 |          5669 |
| req_duration_avg    | 17.39156033 |     19.06719033 |   293.9646488 |
| req_duration_median |     17.1585 |           18.24 |        42.709 |
| req_duration_p90    |      19.011 |         21.5532 |      1169.528 |
| req_duration_p95    |     20.4552 |        27.50975 |     1392.9619 |
</div>

[Benchmark source code](https://github.com/domdomegg/benchmark-next-vs-nginx).

</Details>

## Takeaways

At low load, moving to a remote server increases latency across the board. Surprisingly, Next.js actually starts to do slightly better than nginx: I’m not certain why this is the case. It could be various optimisations like their [image optimisation](https://nextjs.org/docs/pages/building-your-application/optimizing/images) (although I’m not certain this was enabled).

At higher load and measuring HTTP request times, nginx just takes the lead by a couple of milliseconds: but the difference is pretty small (note this is at much lower volumes than yesterday’s post).

This experiment also reinforced my belief that nginx is more of a pain to setup. Next.js ‘just worked’. I cloned the repo, did an `npm install` and things were happy. nginx required fiddling with the config file (I ended up using the absolute path rather than relative path with `PWD` to get it working), all the while being annoying to troubleshoot.

I think [yesterday’s takeaway](../benchmark-next-vs-nginx/#takeaways) still holds: that in practice the performance penalty for picking Next.js over nginx is often worth the setup hassle.

[^1]: Hosted in eu-west-1, while I was sending requests from a high-bandwidth business internet connection in London.
